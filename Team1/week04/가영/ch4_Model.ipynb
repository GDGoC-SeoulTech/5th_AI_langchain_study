{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OZShekyv4e6N"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-teddynote"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "from langchain_teddynote import logging\n",
        "from langchain_teddynote.messages import stream_response\n",
        "\n",
        "# 프로젝트 이름을 입력합니다.\n",
        "logging.langsmith(\"CH04-Models\")"
      ],
      "metadata": {
        "id": "Z-zlxRnHAJNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ChatOpenAI 객체를 생성합니다.\n",
        "gpt = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gpt-4o\",  # 모델명\n",
        ")\n",
        "\n",
        "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
        "answer = gpt.stream(\"사랑이 뭔가요?\")\n",
        "\n",
        "# 답변 출력\n",
        "stream_response(answer)"
      ],
      "metadata": {
        "id": "lwuoDpe3UaPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_teddynote.models import ChatPerplexity\n",
        "os.environ[\"PPLX_API_KEY\"] = userdata.get(\"PPLX_API_KEY\")\n",
        "\n",
        "perplexity = ChatPerplexity(\n",
        "    model=\"llama-3-sonar-large-32k-online\",\n",
        "    temperature=0.2,\n",
        "    top_p=0.9,\n",
        "    search_domain_filter=[\"perplexity.ai\"],\n",
        "    return_images=False,\n",
        "    return_related_questions=True,\n",
        "    # search_recency_filter=\"month\",\n",
        "    top_k=0,\n",
        "    streaming=False,\n",
        "    presence_penalty=0,\n",
        "    frequency_penalty=1,\n",
        ")\n",
        "\n",
        "# 응답 출력\n",
        "response = perplexity.invoke(\"사랑이 뭔가요?\")\n",
        "print(response.content)\n",
        "\n",
        "print()\n",
        "for i, citation in enumerate(response.citations):\n",
        "    print(f\"[{i+1}] {citation}\")\n"
      ],
      "metadata": {
        "id": "NwDIoRDzYIF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cache"
      ],
      "metadata": {
        "id": "Jq4YEzCohrcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# 모델을 생성합니다.\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# 프롬프트를 생성합니다.\n",
        "prompt = PromptTemplate.from_template(\"{country} 에 대해서 200자 내외로 요약해줘\")\n",
        "\n",
        "# 체인을 생성합니다.\n",
        "chain = prompt | llm\n"
      ],
      "metadata": {
        "id": "Ss5wtsv6htzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = chain.invoke({\"country\": \"한국\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "aQ4ZFnVdiEDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### InMemoryCache"
      ],
      "metadata": {
        "id": "GPylaUtxiftn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dbd1ebb5"
      },
      "source": [
        "!pip install -U langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07bb6c5f"
      },
      "source": [
        "%%time\n",
        "from langchain.globals import set_llm_cache\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "# 인메모리 캐시를 사용합니다.\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# 체인을 실행합니다.\n",
        "response = chain.invoke({\"country\": \"한국\"})\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 체인을 실행합니다.\n",
        "response = chain.invoke({\"country\": \"한국\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "b_h7wta8jRgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SQLite Cache"
      ],
      "metadata": {
        "id": "4JwsjzJhjr7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.cache import SQLiteCache\n",
        "from langchain_core.globals import set_llm_cache\n",
        "import os\n",
        "\n",
        "# 캐시 디렉토리를 생성합니다.\n",
        "if not os.path.exists(\"cache\"):\n",
        "    os.makedirs(\"cache\")\n",
        "\n",
        "# SQLiteCache를 사용합니다.\n",
        "set_llm_cache(SQLiteCache(database_path=\"cache/llm_cache.db\"))"
      ],
      "metadata": {
        "id": "aBXHMusLjtqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 체인을 실행합니다.\n",
        "response = chain.invoke({\"country\": \"한국\"})\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "8dtt0XKZkKa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 체인을 실행합니다.\n",
        "response = chain.invoke({\"country\": \"한국\"})\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "KlccaZkZkM6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPU time은 줄었지만 Wall time은 오히려 늘어난 현상이 나타났습니다. 이는 캐싱(Caching) 효과가 나타났음에도 불구하고, 다른 외부 요인 때문에 **전체 시간(Wall time)**이 더 오래 걸렸음을 의미"
      ],
      "metadata": {
        "id": "JHU79Lyfkp8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 모델 직렬화(Serialization) - 저장 및 불러오기"
      ],
      "metadata": {
        "id": "CsrFVhdBkr2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_lc_serializable 클래스 메서드로 실행하여 LangChain 클래스가 직렬화 가능한지 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "WzTIoMtulRBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 구글 생성 AI"
      ],
      "metadata": {
        "id": "rOEJLir9pVwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "id": "Wm1aiIYNpoDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# ChatGoogleGenerativeAI 언어 모델을 초기화합니다.\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
        "\n",
        "# 프롬프트를 전달하여 결과를 생성합니다.\n",
        "answer = llm.stream(\"자연어처리에 대해서 간략히 설명해 줘\")\n",
        "\n",
        "# 결과를 출력합니다.\n",
        "stream_response(answer)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "An-kU1ePpa6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 허깅페이스"
      ],
      "metadata": {
        "id": "fKdm_HKQ9Owu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU huggingface_hub"
      ],
      "metadata": {
        "id": "6znQR6UY9Z9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 코랩 Secret에 저장된 키를 환경 변수에 설정\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get(\"LANGSMITH_API_KEY\")"
      ],
      "metadata": {
        "id": "JTNYi-BeyL7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "28A2yJXm8jK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"<|system|>\n",
        "You are a helpful assistant.<|end|>\n",
        "<|user|>\n",
        "{question}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "LceKji5e8mAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Serverless Endpoints"
      ],
      "metadata": {
        "id": "dbbsGr4699_j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9164407c"
      },
      "source": [
        "!pip install -qU langchain-huggingface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "설치하신 패키지들은 각각 다른 역할을 합니다.\n",
        "\n",
        "huggingface_hub: Hugging Face Hub에 접근하고 모델, 데이터셋 등을 다운로드하는 데 필요한 라이브러리입니다.\n",
        "langchain-teddynote: LangChain과 관련된 특정 기능이나 예제를 제공하는 라이브러리입니다.\n",
        "langchain-huggingface: LangChain 프레임워크에서 Hugging Face 모델을 직접적으로 사용할 수 있도록 연결해주는 역할을 합니다.\n",
        "따라서 langchain-huggingface 패키지를 설치해야 LangChain 코드에서 HuggingFaceEndpoint와 같은 클래스를 사용하여 Hugging Face 모델을 불러오고 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "zIbzIeeF_LCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "repo_id 변수에 HuggingFace 모델의 repo ID(저장소 ID) 를 할당"
      ],
      "metadata": {
        "id": "23LsS9m7-InN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# 사용할 모델의 저장소 ID를 설정합니다.\n",
        "repo_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.\n",
        "    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정합니다.\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # 허깅페이스 토큰\n",
        ")\n",
        "\n",
        "# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.\n",
        "response = chain.invoke({\"question\": \"what is the capital of South Korea?\"})\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "BnVd_XWT-JCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "973c0d70"
      },
      "source": [
        "!pip install -U langchain-core langchain langchain-community langchain-huggingface langchain-openai langchain-google-genai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf3bd829"
      },
      "source": [
        "import os\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Re-define the prompt template if it was cleared\n",
        "template = \"\"\"<|system|>\n",
        "You are a helpful assistant.<|end|>\n",
        "<|user|>\n",
        "{question}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "# 사용할 모델의 저장소 ID를 설정합니다.\n",
        "repo_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# 환경 변수에서 허깅페이스 토큰을 가져옵니다.\n",
        "# Colab Secrets에 HUGGINGFACEHUB_API_TOKEN으로 저장되어 있어야 합니다.\n",
        "hf_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "# 토큰이 없으면 오류 메시지를 출력합니다.\n",
        "if not hf_token:\n",
        "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN not found in environment variables.\")\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.\n",
        "    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정합니다.\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=hf_token,  # 허깅페이스 토큰\n",
        ")\n",
        "\n",
        "# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.\n",
        "response = chain.invoke({\"question\": \"what is the capital of South Korea?\"})\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 허깅페이스 로컬(HuggingFace Local)"
      ],
      "metadata": {
        "id": "G-7Bmx26L3tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "로컬 환경에서 허깅페이스 모델(Phi-3-mini-4k-instruct)을 불러와 랭체인(HuggingFacePipeline)을 통해 텍스트 생성 추론을 수행하는 과정 -> 아래 코드들"
      ],
      "metadata": {
        "id": "89CihfSyL6Gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Ollama"
      ],
      "metadata": {
        "id": "OynBdfZTWGss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelfile 로부터 커스텀 모델 생성하기\n",
        "\n",
        "모델을 임포트하기 위해 ModelFile을 먼저 생성해야 합니다."
      ],
      "metadata": {
        "id": "2foNMpuFWNLX"
      }
    }
  ]
}
