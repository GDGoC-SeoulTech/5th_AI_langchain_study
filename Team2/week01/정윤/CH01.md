# LangChain이란 무엇인가
LLM 기반의 애플리케이션을 구축하기 위한 프레임워크.
상호운영 가능한 구성 요소 및 서드파티와의 연결(chain)을 통해 AI 애플리케이션의 개발을 간소화.

Context: LangChain을 활용해 LLM에 다양한 소스와 연결해 context를 인식 및 유지해 일관된 응답 성능 확보 가능.

Inference: 주어진 context를 바탕으로 어떤 응답/액션을 취할지 스스로 결정. 종합적인 상황 판단을 통해 현 상황에서의 적절한 해결책을 제시.

# LangChain 구성
LangChain Library: Python, JS 라이브러리. 컴포넌트의 인터페이스 통합/컴포넌트를 체인과 에이전트로 결합하는 런타임/체인과 에이전트의 구현
https://python.langchain.com/v0.2/docs/introduction/

LangChain Template: 참조 아키텍처. https://templates.langchain.com/

LangServe: REST API 배포를 위한 라이브러리
https://github.com/langchain-ai/langserve

LangSmith: LLM 프레임워크에 구축된 체인을 디버그, 테스트, 평가, 모니터할 수 있는 LangChain 통합 개발자 플랫폼.
https://smith.langchain.com/

LangGraph: LangChain 기반 상태유지 가능한 다중 액터 애플리케이션 구축을 위한 라이브러리. 다중 체인(액터)을 순환 방식 조정 능력을 LangChain 표현 언어에 추가.
https://langchain-ai.github.io/langgraph/

주요 모듈
모델 I/O: 프롬프트 관리와 최적화 / LLM과의 일반적 인터페이스 작업을 위한 유틸리티 등
검색: 생성 단계에서 필요한 데이터 외부 데이터 소스에서 조달
에이전트: LLM의 행동을 결정. 행동을 실행, 관찰, 반복.

# 환경 구성
https://teddynote.com/10-RAG%EB%B9%84%EB%B2%95%EB%85%B8%ED%8A%B8/%ED%99%98%EA%B2%BD%20%EC%84%A4%EC%A0%95%20(Windows)/

로컬 환경에 기본적으로 anaconda3 기반의 python 3.13 버전이 설치되어 있기 때문에, 파이썬 버전 충돌을 방지하기 위해 환경 설정 가이드에 없는 내용을 추가로 진행.

conda와 pyenv 모두 일종의 python 가상 환경을 구축하고 관리할 수 있음.
프로젝트마다 별도의 python 버전, 패키지를 관리할 수 있음.

```shell
> pyenv install 3.11
```
설치 이후, LangChain을 실습할 프로젝트 폴더로 이동한 이후에 다음과 같은 명령어를 실행해 해당 프로젝트에 한해서만 pyenv의 python 3.11을 사용하도록 설정.
```shell
> pyenv local 3.11
> python --version
```

conda가 activate된 상태에서는 anaconda의 버전을 따라가기 때문에,
```
> conda deactivate
```
명령어를 실행해 pyenv에 접근할 수 있도록 해야함.

기본 python이 anaconda로 되어 있기 때문에 conda가 자동 실행되는 것을 방지하기 위한 조치를 취함.
```
> conda config --set auto_activate_base false
```

이후 가이드를 따라 모든 poetry 설치와 가상환경 구축을 마무리하면 다음과 같이 구분된 파이썬 환경을 만들 수 있음.

```
poetry shell 활성화 || 프로젝트 폴더 내부
-> Python 3.11.9

poetry shell exit && 프로젝트 폴더 외부
-> Python 지정 안됨

conda activate
-> Python 3.13.5
```

LangChain 프로젝트 폴더에서만 사용할 python 3.11.9를 별도로 관리할 수 있음.

# LogProb
Log Probability, 로그 확률.
어떤 모델이 특정 토큰 또는 출력 시퀀스를 생성할 확률의 로그값.

모델이 어떠한 문장을 생성할 때 사용한 각 토큰은 개별 확률 값을 가짐.
```I love cats```라는 문장에 사용된 토큰 ```I```, ```love```, ```cats```는 고유의 확률값은 갖는데, 해당 값에 로그를 적용한 값을 LogProb을 통해 확인할 수 있다.

토큰이 갖는 확률의 의미: 언어모델은 이전까지의 문맥을 바탕으로 다음에 올 토큰(단어 조각)의 확률 분포를 예측해서 문장을 생성함. ```cats```의 확률은 ```I love``` 뒤에 수많은 토큰 후보 중에 ```cats```라는 단어가 올 가능성을 계산한 값. 확률이 높을수록 언어모델이 판단하기에 현재 문맥에서 가장 어울리는 토큰이라는 의미.

왜 Log를 적용하는가: 확률의 곱연산은 매우 작은 값이기 때문에, log를 통해 덧셈으로 변환해 안정적인 연산이 가능함. 

왜 필요한가:
출력의 신뢰도 평가 - logprob이 낮으면 모델이 자신 없는 응답이라고 판단 가능. 모델 응답에 대한 필터링/재생성 가능

비교 생성 - 모델이 문장의 집합 중에서 어떤 것이 더 자연스럽다고 여기는지 logprob 합 비교로 판단 가능

LangSmith 추적 - logprob을 LangSmith trace에 남기면 모델의 의사결정에 과정에 대한 디버깅 가능.

# 스트리밍 출력
```python
# 스트리밍 출력을 위한 요청
answer = chain.stream({"question": "미국에서 피자 주문"})
# 스트리밍 출력
stream_response(answer)
```
궁금증:
파이썬이 기본적으로 인터프리터 언어인데, 어떻게 실시간 출력이 가능한가?
```chain.stream(...)```의 리턴값이 모두 ```answer```에 저장된 이후에 ```stream_response()```가 실행되는 것이 일반적인 형태.

답변:
그러나 ```chain.stream(...)```은 파이썬 제너레이터 or 비동기 스트리밍 방식으로 구현. 데이터를 chunk 단위로 생성 및 반환하기 때문에 ```answer```는 모든 데이터를 한 번에 처리하지 않고, 스트리밍 도중에 반환된 데이터를 바로 사용할 수 있음. 전체 데이터를 메모리에 로드하지 않고 한번에 하나의 chunk만 리턴.

```chain_stream()```을 호출해 스트리밍 데이터 반환하는 제네레이터 객체 생성
```stream_response()```가 제네레이터로부터 데이터를 하나씩 받아옴.
스트리밍 데이터가 생성될때마다 ```stream_response()```가 이를 처리 및 출력.
결과적으로 사용자 입장에서 실시간으로 응답이 출려되는 것처럼 보임!

# System Prompt v. User Prompt
"System": 모델의 성격, 말투, 역할, 규칙 설정. 대화 전체에 기본 제약조건. => 컨텍스트 맨 앞에 삽입되어 모델의 일관된 성격을 유지하고 개발자가 정의한 기능적 역할 부여할 수 있음.
"User": 사용자의 질문, 명령, 요청 전달. 시스템 지침을 준수하면서 구체적인 요청 수행.

# 프롬프트 템플릿
```python
# 프롬프트 정의 및 객체 생성
prompt_template = PromptTemplate.from_template("What is {var}?")

# 프롬프트 생성
prompt = prompt_template.format(var="LangChain")
```

# Chain 생성
```python
# 프롬프트 템플릿을 이용하여 프롬프트를 생성.
prompt = PromptTemplate.from_template(template)

# 모델 설정정.
model = ChatOpenAI(model_name="gpt-4.1-nano")

# 문자열 출력 파서 초기화.
output_parser = StrOutputParser()

# 체인 생성성
chain = prompt | model | output_parser

input = {"var": "LangChain"}
chain.invoke(input)
answer = chain.stream(input)
```
## output parser
모델의 출력 데이터 처리 및 원하는 형식으로 변환/후처리.
자연어 텍스트 뿐만 아니라, JSON, 리스트, 딕셔너리 등의 형태로 모델의 응답을 재구성할 수 있음.

## invoke() || stream()
invoke() || stream() 통해 딕셔너리 형태로 입력값을 전달

# LCEL 인터페이스
Runnable: 호출, 일괄 처리, 스트리밍, 변환 및 구성이 가능한 작업 단위.
invoke/ainvoke: 단일 입력을 출력으로 변환
```python
chain.invoke({"var": "LangChain"})

answer = chain.ainvoke({"var": "LangChain"})
await answer
```

batch/abatch: 여러 입력을 효율적으로 출력으로 변환
```python
chain.batch(
    [
        {"var": "LangChain"},
        {"var": "LangSmith"},
        {"var": "LangServe"},
        {"var": "LangGraph"},
    ],
    config={"max_concurrency": 3}
)

answer = chain.abatch(
    [
        {"var": "LangChain"},
        {"var": "LangSmith"},
        {"var": "LangServe"},
        {"var": "LangGraph"},
    ],
    config={"max_concurrency": 3}
)
await answer
```

stream/astream: 단일 입력에서 생성되는 대로 출력을 스트림으로 나타냄.
```python
for token in chain.stream({"var": "LangChain"}):
    print(token, flush=True)

async for token in chain.astream({"var": "LangChain"}):
    print(token, flush=True)
```

> a 접두사가 붙은 메서드는 비동기 대응.

# Parallel
여러 개의 체인을 동시에 생성 및 실행.
```python
chain1 = (
    prompt | model | parser
)

chain2 = {
    prompt | model | parser
}

combined = RunnableParallel(one=chain1, two=chain2)
combined.invoke({"var": "LangChain"})
combined.batch([{"var": "LangChain"}, {"var": "LangSmith"}])
```

# RunnablePassThrough
`RunnablePassThrough`로 입력 변경하지 않거나 추가 키 더해 전달 가능.
단독 사용 시 단순히 입력을 받아 전달.
`RunnablePassThrough.assign()`을 통해 입력을 받아 assign 함수에 전달된 추가 인수를 추가

```python
runnable_chain = {"num": RunnablePassThrough()} | model | parser

runnable_chain.invoke(10) # {"num": 10}}

(RunnablePassthrough.assign(new_num=lambda x: x["num"] * 3)).invoke({"num": 1})
# {'num': 1, 'new_num': 3}
```

# RunnableLambda
사용자 정의 함수를 매핑.
```python
def get_today(a):
    return datetime.today().strftime("%b-%d")

prompt = PromptTemplate.from_template(
    "{today} 가 생일인 유명인 {n} 명을 나열하세요. 생년월일을 표기해 주세요."
)
llm = ChatOpenAI(temperature=0, model_name="gpt-4.1-mini")

chain = (
    # get_today라는 사용자 정의함수를 딕셔너리에 전달.
    {"today": RunnableLambda(get_today), "n": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

print(chain.invoke(3))
# 입력값 3이 RunnableLamdba의 함수 입력과 RunnablePassThrough에 각각 전달되어
# {"today": get_today(3), "n": 3}의 형태로 prompt에 전달됨.
```